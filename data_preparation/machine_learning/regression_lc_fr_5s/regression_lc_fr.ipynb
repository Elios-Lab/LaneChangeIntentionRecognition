{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input,Conv1D, LayerNormalization,MultiHeadAttention, GlobalAveragePooling1D, Embedding,MaxPooling1D, LSTM, Dense, Dropout, Flatten, GRU\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=\"../data/\"\n",
    "data_prepared=\"data_prepared/\"\n",
    "classes=2\n",
    "num_trials=50\n",
    "epochs=200\n",
    "patience=15\n",
    "window_length=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_prepared):\n",
    "    os.makedirs(data_prepared)\n",
    "\n",
    "recreate_data = not os.path.exists(data_prepared+\"x_train.pkl\")\n",
    "if recreate_data:\n",
    "    # Step 1: Load the pickle files\n",
    "    with open(data_folder+\"x_lc_training\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        x_lc_training = pickle.load(f)\n",
    "    with open(data_folder+\"x_fr_training\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        x_fr_training = pickle.load(f)\n",
    "    with open(data_folder+\"y_lc_training\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        y_lc_training = pickle.load(f)\n",
    "    with open(data_folder+\"y_fr_training\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        y_fr_training = pickle.load(f)\n",
    "        \n",
    "    with open(data_folder+\"x_lc_validation\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        x_lc_validation = pickle.load(f)\n",
    "    with open(data_folder+\"x_fr_validation\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        x_fr_validation = pickle.load(f)\n",
    "    with open(data_folder+\"y_lc_validation\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        y_lc_validation = pickle.load(f)\n",
    "    with open(data_folder+\"y_fr_validation\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        y_fr_validation = pickle.load(f)\n",
    "    \n",
    "    with open(data_folder+\"x_lc_testing\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        x_lc_testing = pickle.load(f)\n",
    "    with open(data_folder+\"x_fr_testing\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        x_fr_testing = pickle.load(f)\n",
    "    with open(data_folder+\"y_lc_testing\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        y_lc_testing = pickle.load(f)\n",
    "    with open(data_folder+\"y_fr_testing\"+str(window_length)+\".pkl\", \"rb\") as f:\n",
    "        y_fr_testing = pickle.load(f)\n",
    "\n",
    "    # Step 4: Combine `lc` and `fr` for training, validation, and testing\n",
    "    x_train = np.vstack((x_lc_training, x_fr_training))\n",
    "    y_train = np.hstack((y_lc_training, y_fr_training))\n",
    "\n",
    "    x_val = np.vstack((x_lc_validation, x_fr_validation))\n",
    "    y_val = np.hstack((y_lc_validation, y_fr_validation))\n",
    "\n",
    "    x_test = np.vstack((x_lc_testing, x_fr_testing))\n",
    "    y_test = np.hstack((y_lc_testing, y_fr_testing))\n",
    "    \n",
    "    # Step 1: Concatenate the windows for each dataset along the time axis\n",
    "    x_train_combined = np.concatenate(x_train, axis=0)  # Combine all train windows into a single 2D array\n",
    "    x_val_combined = np.concatenate(x_val, axis=0)      # Combine all val windows\n",
    "    x_test_combined = np.concatenate(x_test, axis=0)    # Combine all test windows\n",
    "\n",
    "    # Step 2: Apply scaling\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled_combined = scaler.fit_transform(x_train_combined)  # Fit and scale training data\n",
    "    x_val_scaled_combined = scaler.transform(x_val_combined)          # Scale validation data\n",
    "    x_test_scaled_combined = scaler.transform(x_test_combined)        # Scale test data\n",
    "    \n",
    "    #create folder data_prepared\n",
    "    if not os.path.exists(data_prepared):\n",
    "        os.makedirs(data_prepared)\n",
    "    #save scaler\n",
    "    joblib.dump(scaler, data_prepared+\"scaler.pkl\")\n",
    "    \n",
    "    # Step 3: Split back into windows of 10x30\n",
    "    x_train = x_train_scaled_combined.reshape(-1, window_length, 30)  # Reshape back to windows\n",
    "    x_val = x_val_scaled_combined.reshape(-1, window_length, 30)      # Reshape back to windows\n",
    "    x_test = x_test_scaled_combined.reshape(-1, window_length, 30)    # Reshape back to windows\n",
    "    \n",
    "    #zip the data and shuffle\n",
    "    np.random.shuffle(list(zip(x_train, y_train)))\n",
    "    np.random.shuffle(list(zip(x_val, y_val)))\n",
    "    np.random.shuffle(list(zip(x_test, y_test)))\n",
    "   \n",
    "    x_train_reshaped = [window.reshape(-1) for window in x_train]\n",
    "    x_val_reshaped = [window.reshape(-1) for window in x_val]\n",
    "    x_test_reshaped = [window.reshape(-1) for window in x_test]\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "    y_test = np.array(y_test)\n",
    "     \n",
    "    # Step 6: Save the scaled datasets and corresponding labels\n",
    "    with open(data_prepared+\"x_train.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_train, f)\n",
    "    with open(data_prepared+\"x_train_reshaped.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_train_reshaped, f)\n",
    "    with open(data_prepared+\"y_train.pkl\", \"wb\") as f:\n",
    "        pickle.dump(y_train, f)\n",
    "\n",
    "    with open(data_prepared+\"x_val.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_val, f)\n",
    "    with open(data_prepared+\"x_val_reshaped.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_val_reshaped, f)\n",
    "    with open(data_prepared+\"y_val.pkl\", \"wb\") as f:\n",
    "        pickle.dump(y_val, f)\n",
    "\n",
    "    with open(data_prepared+\"x_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_test, f)\n",
    "    with open(data_prepared+\"x_test_reshaped.pkl\", \"wb\") as f:\n",
    "        pickle.dump(x_test_reshaped, f)\n",
    "    with open(data_prepared+\"y_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump(y_test, f)\n",
    "\n",
    "    print(\"Data successfully split, combined, scaled, and saved.\")\n",
    "\n",
    "else:\n",
    "    x_train = pickle.load(open(data_prepared+\"x_train.pkl\", \"rb\"))\n",
    "    x_train_reshaped = pickle.load(open(data_prepared+\"x_train_reshaped.pkl\", \"rb\"))\n",
    "    y_train = pickle.load(open(data_prepared+\"y_train.pkl\", \"rb\"))\n",
    "    x_val = pickle.load(open(data_prepared+\"x_val.pkl\", \"rb\"))\n",
    "    x_val_reshaped = pickle.load(open(data_prepared+\"x_val_reshaped.pkl\", \"rb\"))\n",
    "    y_val = pickle.load(open(data_prepared+\"y_val.pkl\", \"rb\"))\n",
    "    x_test = pickle.load(open(data_prepared+\"x_test.pkl\", \"rb\"))\n",
    "    x_test_reshaped = pickle.load(open(data_prepared+\"x_test_reshaped.pkl\", \"rb\"))\n",
    "    y_test = pickle.load(open(data_prepared+\"y_test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"models\")):\n",
    "    os.makedirs(\"models\")\n",
    "model_folder_path = \"./models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_models(tuner, model_name, num_models=2, retrain_on_full_data=False, early_stop=None):\n",
    "    \"\"\"\n",
    "    Evaluates the top models found by the tuner on validation and test sets, and returns the best model based on test accuracy.\n",
    "    \n",
    "    Args:\n",
    "        tuner: Keras Tuner object that performed the hyperparameter search.        \n",
    "        num_models: Number of top models to evaluate (default is 5).\n",
    "        retrain_on_full_data: If True, retrains each model on combined x_train and x_val data.\n",
    "        early_stop: Optional callback for early stopping during retraining.\n",
    "    \n",
    "    Returns:\n",
    "        The best model based on test accuracy.\n",
    "    \"\"\"\n",
    "    # Retrieve the top models and their best hyperparameters\n",
    "    best_models = tuner.get_best_models(num_models=num_models)\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters(num_trials=num_models)\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "    if(not os.path.exists(model_folder_path+model_name)):\n",
    "            os.makedirs(model_folder_path+model_name)\n",
    "\n",
    "    for i, (model, hyperparams) in enumerate(zip(best_models, best_hyperparameters), start=1):\n",
    "        print(f\"\\n--- Evaluation of Model {i} ---\")\n",
    "        \n",
    "        # Print the best hyperparameters for this model\n",
    "        print(\"Best Hyperparameters:\")\n",
    "        for param, value in hyperparams.values.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        #save best hyperparameters to file        \n",
    "        with open(model_folder_path+model_name+\"/best_hyperparameters_model_\"+str(i)+\".txt\", 'w') as f:\n",
    "            f.write(\"Best Hyperparameters:\\n\")\n",
    "            for param, value in hyperparams.values.items():\n",
    "                f.write(f\"{param}: {value}\\n\")\n",
    "            f.close()\n",
    "        \n",
    "        # Optionally, retrain the model on combined training and validation data\n",
    "        if retrain_on_full_data and x_train is not None and y_train is not None and x_val is not None and y_val is not None:\n",
    "            x_train_validation = np.concatenate((x_train, x_val), axis=0)\n",
    "            y_train_validation = np.concatenate((y_train, y_val), axis=0)            \n",
    "            if early_stop and early_stop.monitor == 'val_loss':\n",
    "                retrain_early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=early_stop.patience, restore_best_weights=early_stop.restore_best_weights)\n",
    "            else:\n",
    "                retrain_early_stop = early_stop                \n",
    "            model.fit(x_train_validation, y_train_validation, epochs=50, batch_size=64, callbacks=[retrain_early_stop] if retrain_early_stop else None)\n",
    "        \n",
    "        # Evaluate on validation set if provided        \n",
    "        val_loss, val_mse, val_mae = model.evaluate(x_val, y_val, verbose=0)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_mse, test_mae= model.evaluate(x_test, y_test, verbose=0)\n",
    "        \n",
    "        # Generate classification report and confusion matrix for the test set\n",
    "        y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "        r_squared= r2_score(y_test,y_pred)\n",
    "        mae= mean_absolute_error(y_test,y_pred)\n",
    "        # Save results\n",
    "        results.append({\n",
    "            \"Model\": f\"Best_Model_{i}\",\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation mse\":val_mse,\n",
    "            \"Validation mae\": val_mae,\n",
    "            \"Testing Loss\": test_loss,\n",
    "            \"Testing mse\":test_mse,\n",
    "            \"Testing mae\": test_mae,\n",
    "            \"rmse\":rmse,\n",
    "            \"r_squared\":r_squared,\n",
    "            \"mae\":mae\n",
    "        })         \n",
    "        \n",
    "    # Create and display a DataFrame with the accuracy results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    display(results_df)  # If using Jupyter, this will show the table directly; otherwise, use `print(results_df)`\n",
    "    \n",
    "    #save results to file\n",
    "    results_df.to_csv(model_folder_path+model_name+\"/best_models_evaluation_results.csv\", index=False)\n",
    "    \n",
    "    # Identify and return the best model based on test accuracy\n",
    "    best_model_idx = results_df['rmse'].idxmin()\n",
    "    best_model = best_models[best_model_idx]\n",
    "    print(f\"\\nBest Model is Model {best_model_idx + 1} with rmse: {results_df['rmse'].min()}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classes==2:\n",
    "    final_neurons=1\n",
    "    activation = 'linear'\n",
    "    loss = 'mean_squared_error'\n",
    "else:\n",
    "    final_neurons=3\n",
    "    activation = 'softmax'\n",
    "    loss = 'sparse_categorical_crossentropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "(165182, 50, 30)\n"
     ]
    }
   ],
   "source": [
    "n_features = x_train.shape[2]\n",
    "print(n_features)\n",
    "print( x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "n_features = x_train.shape[2]  # Ensure x_train has the shape (num_windows, window_size, n_features)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with variable units and dropout rate\n",
    "    model.add(LSTM(\n",
    "        units=hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "        activation='tanh',\n",
    "        input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "        return_sequences=hp.Boolean('use_second_lstm')  # To add additional LSTM layers if needed\n",
    "    ))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Optional second LSTM layer\n",
    "    if hp.Boolean('use_second_lstm'):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Int('units_2', min_value=32, max_value=256, step=32),\n",
    "            activation='tanh',\n",
    "            return_sequences=hp.Boolean('use_third_lstm') \n",
    "        ))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Optional third LSTM layer\n",
    "    if hp.Boolean('use_second_lstm') and hp.Boolean('use_third_lstm'):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Int('units_3', min_value=32, max_value=256, step=32),\n",
    "            activation='tanh',\n",
    "            return_sequences=False  # Last LSTM layer\n",
    "        ))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(final_neurons, activation=activation)) \n",
    "    \n",
    "    # Compile the model with variable hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "        ),\n",
    "        loss=loss,\n",
    "       metrics=[\"mean_squared_error\",\"mean_absolute_error\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from lstm_tuning_2_classes\\1\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_trials=num_trials,  # Maximum number of configurations to test\n",
    "    executions_per_trial=1,  # Repeat each configuration for stability\n",
    "    directory=f'lstm_tuning_{classes}_classes',\n",
    "    project_name=project_name,\n",
    "    overwrite=False  # Set to True to overwrite previous results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for Early Stopping to prevent overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error',mode='min', patience=patience, restore_best_weights=True)\n",
    "\n",
    "# Start hyperparameter search\n",
    "tuner.search(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=256, \n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation of Model 1 ---\n",
      "Best Hyperparameters:\n",
      "units_1: 192\n",
      "use_second_lstm: False\n",
      "dropout_1: 0.4\n",
      "learning_rate: 0.001\n",
      "units_2: 192\n",
      "use_third_lstm: True\n",
      "dropout_2: 0.2\n",
      "units_3: 160\n",
      "dropout_3: 0.30000000000000004\n",
      "1129/1129 [==============================] - 4s 3ms/step\n",
      "\n",
      "--- Evaluation of Model 2 ---\n",
      "Best Hyperparameters:\n",
      "units_1: 32\n",
      "use_second_lstm: True\n",
      "dropout_1: 0.4\n",
      "learning_rate: 0.01\n",
      "units_2: 32\n",
      "use_third_lstm: False\n",
      "dropout_2: 0.4\n",
      "units_3: 96\n",
      "dropout_3: 0.2\n",
      "1129/1129 [==============================] - 6s 5ms/step\n",
      "\n",
      "Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation mse</th>\n",
       "      <th>Validation mae</th>\n",
       "      <th>Testing Loss</th>\n",
       "      <th>Testing mse</th>\n",
       "      <th>Testing mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best_Model_1</td>\n",
       "      <td>0.214909</td>\n",
       "      <td>0.214909</td>\n",
       "      <td>0.262401</td>\n",
       "      <td>0.301612</td>\n",
       "      <td>0.301612</td>\n",
       "      <td>0.301859</td>\n",
       "      <td>0.549192</td>\n",
       "      <td>0.842535</td>\n",
       "      <td>0.301859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best_Model_2</td>\n",
       "      <td>0.230060</td>\n",
       "      <td>0.230060</td>\n",
       "      <td>0.263704</td>\n",
       "      <td>0.341658</td>\n",
       "      <td>0.341658</td>\n",
       "      <td>0.316896</td>\n",
       "      <td>0.584516</td>\n",
       "      <td>0.821628</td>\n",
       "      <td>0.316895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Validation Loss  Validation mse  Validation mae  \\\n",
       "0  Best_Model_1         0.214909        0.214909        0.262401   \n",
       "1  Best_Model_2         0.230060        0.230060        0.263704   \n",
       "\n",
       "   Testing Loss  Testing mse  Testing mae      rmse  r_squared       mae  \n",
       "0      0.301612     0.301612     0.301859  0.549192   0.842535  0.301859  \n",
       "1      0.341658     0.341658     0.316896  0.584516   0.821628  0.316895  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model is Model 1 with rmse: 0.5491921509380875\n"
     ]
    }
   ],
   "source": [
    "best_model = evaluate_best_models(tuner,\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model.save(model_folder_path+\"LSTM/LSTM_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 192)               171264    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 192)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 193       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,457\n",
      "Trainable params: 171,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load model lstm and do model summary\n",
    "loaded_model = tf.keras.models.load_model(model_folder_path+\"LSTM/LSTM_model.h5\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "(165182, 50, 30)\n"
     ]
    }
   ],
   "source": [
    "n_features = x_train.shape[2]\n",
    "print(n_features)\n",
    "print( x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "n_features = x_train.shape[2]  # Ensure x_train has the shape (num_windows, window_size, n_features)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First GRU layer with variable units and dropout rate\n",
    "    model.add(GRU(\n",
    "        units=hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "        activation='tanh',\n",
    "        input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "        return_sequences=hp.Boolean('use_second_gru')  # To add an additional GRU layer if needed\n",
    "    ))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Optional second GRU layer\n",
    "    if hp.Boolean('use_second_gru'):\n",
    "        model.add(GRU(\n",
    "            units=hp.Int('units_2', min_value=32, max_value=256, step=32),\n",
    "            activation='tanh',\n",
    "            return_sequences=hp.Boolean('use_third_gru')\n",
    "        ))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Optional third GRU layer\n",
    "    if hp.Boolean('use_second_gru') and hp.Boolean('use_third_gru'):\n",
    "        model.add(GRU(\n",
    "            units=hp.Int('units_2', min_value=32, max_value=256, step=32),\n",
    "            activation='tanh',\n",
    "            return_sequences=False  # Last GRU layer\n",
    "        ))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "        \n",
    "    # Output layer\n",
    "    model.add(Dense(final_neurons, activation=activation))\n",
    "    \n",
    "    # Compile the model with variable hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "        ),\n",
    "        loss=loss,\n",
    "       metrics=[\"mean_squared_error\",\"mean_absolute_error\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_trials=num_trials,  # Maximum number of configurations to test\n",
    "    executions_per_trial=1,  # Repeat each configuration for stability\n",
    "    directory=f'gru_tuning_{classes}_classes',\n",
    "    project_name=project_name,\n",
    "    overwrite=False  # Set to True to overwrite previous results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 04m 22s]\n",
      "val_mean_absolute_error: 0.2663462460041046\n",
      "\n",
      "Best val_mean_absolute_error So Far: 0.2569979429244995\n",
      "Total elapsed time: 03h 03m 24s\n"
     ]
    }
   ],
   "source": [
    "# Callback for Early Stopping to prevent overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "# Start hyperparameter search\n",
    "tuner.search(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=256, \n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation of Model 1 ---\n",
      "Best Hyperparameters:\n",
      "units_1: 192\n",
      "use_second_gru: True\n",
      "dropout_1: 0.30000000000000004\n",
      "learning_rate: 0.001\n",
      "units_2: 32\n",
      "use_third_gru: True\n",
      "dropout_2: 0.2\n",
      "1129/1129 [==============================] - 9s 7ms/step\n",
      "\n",
      "--- Evaluation of Model 2 ---\n",
      "Best Hyperparameters:\n",
      "units_1: 192\n",
      "use_second_gru: True\n",
      "dropout_1: 0.2\n",
      "learning_rate: 0.001\n",
      "units_2: 32\n",
      "use_third_gru: True\n",
      "dropout_2: 0.2\n",
      "1129/1129 [==============================] - 9s 7ms/step\n",
      "\n",
      "Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation mse</th>\n",
       "      <th>Validation mae</th>\n",
       "      <th>Testing Loss</th>\n",
       "      <th>Testing mse</th>\n",
       "      <th>Testing mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best_Model_1</td>\n",
       "      <td>0.229731</td>\n",
       "      <td>0.229731</td>\n",
       "      <td>0.256998</td>\n",
       "      <td>0.352824</td>\n",
       "      <td>0.352824</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.593990</td>\n",
       "      <td>0.815799</td>\n",
       "      <td>0.311828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best_Model_2</td>\n",
       "      <td>0.226062</td>\n",
       "      <td>0.226062</td>\n",
       "      <td>0.263202</td>\n",
       "      <td>0.322791</td>\n",
       "      <td>0.322791</td>\n",
       "      <td>0.297518</td>\n",
       "      <td>0.568147</td>\n",
       "      <td>0.831478</td>\n",
       "      <td>0.297518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Validation Loss  Validation mse  Validation mae  \\\n",
       "0  Best_Model_1         0.229731        0.229731        0.256998   \n",
       "1  Best_Model_2         0.226062        0.226062        0.263202   \n",
       "\n",
       "   Testing Loss  Testing mse  Testing mae      rmse  r_squared       mae  \n",
       "0      0.352824     0.352824     0.311828  0.593990   0.815799  0.311828  \n",
       "1      0.322791     0.322791     0.297518  0.568147   0.831478  0.297518  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model is Model 2 with rmse: 0.5681468760014271\n"
     ]
    }
   ],
   "source": [
    "best_model = evaluate_best_models(tuner,\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model.save(model_folder_path+\"GRU/GRU_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 50, 192)           129024    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 192)           0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 50, 32)            21696     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50, 32)            0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 32)                6336      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 157,089\n",
      "Trainable params: 157,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load model lstm and do model summary\n",
    "loaded_model = tf.keras.models.load_model(model_folder_path+\"GRU/GRU_model.h5\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = x_train.shape[2]\n",
    "window_size = x_train.shape[1] \n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Number of Conv1D layers\n",
    "    num_conv = hp.Int('num_conv', 1, 2)  # 1 or 2 Conv1D layers\n",
    "    \n",
    "    for i in range(num_conv):\n",
    "        # Number of filters for Conv1D\n",
    "        filters = hp.Int(f'filters_{i+1}', min_value=32, max_value=256, step=32)\n",
    "        \n",
    "        # Kernel size\n",
    "        kernel_size = hp.Choice(f'kernel_size_{i+1}', values=[3, 5, 7])\n",
    "        \n",
    "        # Add Conv1D layer with 'same' padding\n",
    "        if i == 0:\n",
    "            model.add(Conv1D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                input_shape=(window_size, n_features)\n",
    "            ))\n",
    "        else:\n",
    "            # Add additional Conv1D layer\n",
    "            model.add(Conv1D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                activation='relu',\n",
    "                padding='same'\n",
    "            ))\n",
    "        \n",
    "        # MaxPooling1D layer\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        \n",
    "        # Dropout layer\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_conv_{i+1}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Flatten layer to transition to fully connected part\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #put an optional dense\n",
    "    if hp.Boolean('use_dense'):\n",
    "        model.add(Dense(units=hp.Int('units_dense', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_dense', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(final_neurons, activation=activation)) \n",
    "    \n",
    "    # Compile the model with variable hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "        ),\n",
    "        loss=loss,\n",
    "       metrics=[\"mean_squared_error\",\"mean_absolute_error\"]\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from cnn1d_2_cl\\1\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tuner with Bayesian Optimization\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_trials=num_trials,  # Maximum number of configurations to test\n",
    "    executions_per_trial=1,  # Repeat each configuration for stability\n",
    "    directory=f'cnn1d_{classes}_cl',\n",
    "    project_name=project_name,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for Early Stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "# Start hyperparameter search\n",
    "tuner.search(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=256, \n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation of Model 1 ---\n",
      "Best Hyperparameters:\n",
      "num_conv: 2\n",
      "filters_1: 32\n",
      "kernel_size_1: 5\n",
      "dropout_conv_1: 0.4\n",
      "use_dense: True\n",
      "learning_rate: 0.001\n",
      "filters_2: 32\n",
      "kernel_size_2: 7\n",
      "dropout_conv_2: 0.4\n",
      "units_dense: 32\n",
      "dropout_dense: 0.2\n",
      "1129/1129 [==============================] - 1s 996us/step\n",
      "\n",
      "--- Evaluation of Model 2 ---\n",
      "Best Hyperparameters:\n",
      "num_conv: 2\n",
      "filters_1: 64\n",
      "kernel_size_1: 3\n",
      "dropout_conv_1: 0.2\n",
      "use_dense: True\n",
      "learning_rate: 0.001\n",
      "filters_2: 32\n",
      "kernel_size_2: 3\n",
      "dropout_conv_2: 0.4\n",
      "units_dense: 32\n",
      "dropout_dense: 0.2\n",
      "1129/1129 [==============================] - 1s 1ms/step\n",
      "\n",
      "Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation mse</th>\n",
       "      <th>Validation mae</th>\n",
       "      <th>Testing Loss</th>\n",
       "      <th>Testing mse</th>\n",
       "      <th>Testing mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best_Model_1</td>\n",
       "      <td>0.220119</td>\n",
       "      <td>0.220119</td>\n",
       "      <td>0.27851</td>\n",
       "      <td>0.295553</td>\n",
       "      <td>0.295553</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.543647</td>\n",
       "      <td>0.845699</td>\n",
       "      <td>0.310300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best_Model_2</td>\n",
       "      <td>0.214281</td>\n",
       "      <td>0.214281</td>\n",
       "      <td>0.28164</td>\n",
       "      <td>0.330212</td>\n",
       "      <td>0.330212</td>\n",
       "      <td>0.328941</td>\n",
       "      <td>0.574640</td>\n",
       "      <td>0.827604</td>\n",
       "      <td>0.328941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Validation Loss  Validation mse  Validation mae  \\\n",
       "0  Best_Model_1         0.220119        0.220119         0.27851   \n",
       "1  Best_Model_2         0.214281        0.214281         0.28164   \n",
       "\n",
       "   Testing Loss  Testing mse  Testing mae      rmse  r_squared       mae  \n",
       "0      0.295553     0.295553     0.310300  0.543647   0.845699  0.310300  \n",
       "1      0.330212     0.330212     0.328941  0.574640   0.827604  0.328941  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model is Model 1 with rmse: 0.5436473828652598\n"
     ]
    }
   ],
   "source": [
    "best_model = evaluate_best_models(tuner,\"CNN1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model.save(model_folder_path+\"CNN1D/CNN1D_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Bayesian Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.position_embedding = Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
    "        positions = self.position_embedding(positions)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Layer Normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    # Multi-Head Attention\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    # Residual Connection\n",
    "    x = x + inputs\n",
    "\n",
    "    # Feed-Forward Network\n",
    "    x_skip = x\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dense(ff_dim, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    # Residual Connection\n",
    "    return x + x_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (50, 30)\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]  # (window_size, num_features)\n",
    "print(f\"Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci builder con iperspazio per la ricerca\n",
    "def build_transformer_model(hp):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # iperparametri da ottimizzare\n",
    "    head_size = hp.Int(\"head_size\", min_value=32, max_value=128, step=32)\n",
    "    num_heads = hp.Int(\"num_heads\", 1, 8, step=1)\n",
    "    ff_dim = hp.Int(\"ff_dim\", 64, 256, step=64)\n",
    "    num_blocks = hp.Int(\"num_transformer_blocks\", 1, 4, step=1)\n",
    "    dropout = hp.Float(\"dropout\", 0.0, 0.5, step=0.1)\n",
    "    mlp_units = hp.Int(\"mlp_units\", min_value=32, max_value=256, step=32)\n",
    "    mlp_dropout = hp.Float(\"mlp_dropout\", 0.0, 0.5, step=0.1)\n",
    "    lr = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    x = PositionalEncoding(sequence_length=input_shape[0],\n",
    "                           embed_dim=input_shape[1])(inputs)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(mlp_units, activation=\"relu\")(x)\n",
    "    x = Dropout(mlp_dropout)(x)\n",
    "    outputs = Dense(final_neurons, activation=activation)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=loss,\n",
    "        metrics=[\"mean_squared_error\", \"mean_absolute_error\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanzia il tuner con Bayesian Optimization\n",
    "tuner = BayesianOptimization(\n",
    "    build_transformer_model,\n",
    "    objective=\"val_mean_squared_error\",\n",
    "    max_trials=num_trials,\n",
    "    seed=42,\n",
    "    directory=f\"transformer_{classes}_cl\",\n",
    "    project_name=project_name,\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "# Callback per early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=patience, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 16m 30s]\n",
      "val_mean_squared_error: 0.21584130823612213\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.20337837934494019\n",
      "Total elapsed time: 17h 14m 54s\n"
     ]
    }
   ],
   "source": [
    "# Avvia la ricerca degli iperparametri\n",
    "tuner.search(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.2978 | Test MSE: 0.2603 | Test Loss: 0.2603\n"
     ]
    }
   ],
   "source": [
    "# Se vuoi, valuta e salva\n",
    "test_loss, test_mse, test_mae = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {test_mae:.4f} | Test MSE: {test_mse:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "\n",
      "--- Evaluation of Model 1 ---\n",
      "Best Hyperparameters:\n",
      "head_size: 128\n",
      "num_heads: 1\n",
      "ff_dim: 128\n",
      "num_transformer_blocks: 2\n",
      "dropout: 0.0\n",
      "mlp_units: 160\n",
      "mlp_dropout: 0.1\n",
      "learning_rate: 0.001\n",
      "1129/1129 [==============================] - 4s 4ms/step\n",
      "\n",
      "--- Evaluation of Model 2 ---\n",
      "Best Hyperparameters:\n",
      "head_size: 32\n",
      "num_heads: 5\n",
      "ff_dim: 64\n",
      "num_transformer_blocks: 1\n",
      "dropout: 0.0\n",
      "mlp_units: 64\n",
      "mlp_dropout: 0.30000000000000004\n",
      "learning_rate: 0.01\n",
      "1129/1129 [==============================] - 3s 3ms/step\n",
      "\n",
      "--- Evaluation of Model 3 ---\n",
      "Best Hyperparameters:\n",
      "head_size: 128\n",
      "num_heads: 1\n",
      "ff_dim: 64\n",
      "num_transformer_blocks: 2\n",
      "dropout: 0.2\n",
      "mlp_units: 224\n",
      "mlp_dropout: 0.30000000000000004\n",
      "learning_rate: 0.001\n",
      "1129/1129 [==============================] - 4s 4ms/step\n",
      "\n",
      "Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation mse</th>\n",
       "      <th>Validation mae</th>\n",
       "      <th>Testing Loss</th>\n",
       "      <th>Testing mse</th>\n",
       "      <th>Testing mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best_Model_1</td>\n",
       "      <td>0.203378</td>\n",
       "      <td>0.203378</td>\n",
       "      <td>0.275383</td>\n",
       "      <td>0.260263</td>\n",
       "      <td>0.260263</td>\n",
       "      <td>0.297840</td>\n",
       "      <td>0.510160</td>\n",
       "      <td>0.864123</td>\n",
       "      <td>0.297840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best_Model_2</td>\n",
       "      <td>0.207795</td>\n",
       "      <td>0.207795</td>\n",
       "      <td>0.291974</td>\n",
       "      <td>0.280810</td>\n",
       "      <td>0.280810</td>\n",
       "      <td>0.328871</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.853395</td>\n",
       "      <td>0.328871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best_Model_3</td>\n",
       "      <td>0.209153</td>\n",
       "      <td>0.209153</td>\n",
       "      <td>0.291513</td>\n",
       "      <td>0.295441</td>\n",
       "      <td>0.295441</td>\n",
       "      <td>0.328711</td>\n",
       "      <td>0.543545</td>\n",
       "      <td>0.845757</td>\n",
       "      <td>0.328711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Validation Loss  Validation mse  Validation mae  \\\n",
       "0  Best_Model_1         0.203378        0.203378        0.275383   \n",
       "1  Best_Model_2         0.207795        0.207795        0.291974   \n",
       "2  Best_Model_3         0.209153        0.209153        0.291513   \n",
       "\n",
       "   Testing Loss  Testing mse  Testing mae      rmse  r_squared       mae  \n",
       "0      0.260263     0.260263     0.297840  0.510160   0.864123  0.297840  \n",
       "1      0.280810     0.280810     0.328871  0.529915   0.853395  0.328871  \n",
       "2      0.295441     0.295441     0.328711  0.543545   0.845757  0.328711  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model is Model 1 with rmse: 0.5101597554583367\n"
     ]
    }
   ],
   "source": [
    "best_model = evaluate_best_models(tuner,\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/Transformer/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/Transformer/model/assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(model_folder_path+\"Transformer/model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_TS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
